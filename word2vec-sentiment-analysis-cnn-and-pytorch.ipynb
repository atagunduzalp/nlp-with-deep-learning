{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_set = pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv\")\n# Veri setindeki pozitif ve negatif duyguları eşit almak amacıyla yapılmış bir işlem.\ntop_data_df_positive = train_set[train_set['label'] == 0].head(15000)\ntop_data_df_negative = train_set[train_set['label'] == 1].head(15000)\n\ntrain = pd.concat([top_data_df_positive, top_data_df_negative])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom gensim.utils import simple_preprocess\ntrain['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in train['text']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming\nThis part is for streaming the words. This process is for removing the commoner morphological and inflexional endings from words in English."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.parsing.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\ntrain['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in train['tokenized_text']]\ntrain\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train - Test Split \nHere, seperation of dataset as train and test. %70 is going to be train, rest of them will be test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Train Test Split Function\ndef split_train_test(train, test_size=0.3, shuffle_state=True):\n    X_train, X_test, Y_train, Y_test = train_test_split(train[['stemmed_tokens']], \n                                                        train['label'], \n                                                        shuffle=shuffle_state,\n                                                        test_size=test_size, \n                                                        random_state=15)\n    print(\"Value counts for Train sentiments\")\n    print(Y_train.value_counts())\n    print(\"Value counts for Test sentiments\")\n    print(Y_test.value_counts())\n    print(type(X_train))\n    print(type(Y_train))\n    X_train = X_train.reset_index()\n    X_test = X_test.reset_index()\n    Y_train = Y_train.to_frame()\n    Y_train = Y_train.reset_index()\n    Y_test = Y_test.to_frame()\n    Y_test = Y_test.reset_index()\n    print(X_train.head())\n    return X_train, X_test, Y_train, Y_test\n\n# Call the train_test_split\nX_train, X_test, Y_train, Y_test = split_train_test(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word2Vec Model Creation\nThis part calls the function create word vectors with word2vec method.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n# embedding vector size.\nsize = 500\n# size of look for how many words around the selected word.\nwindow = 3\n# occurence of the word in order to take a place in word vector dict.\nmin_count = 1\nworkers = 3\n# 0 for CBOW, 1 for skip-gram\nsg = 0\nOUTPUT_FOLDER = '/kaggle/working/'\n# Function to train word2vec model\ndef make_word2vec_model(train, padding, sg, min_count, size, workers, window):\n    if  padding:\n        #print(len(train))\n        temp_df = pd.Series(train['stemmed_tokens']).values\n        temp_df = list(temp_df)\n        temp_df.append(['pad'])\n        #print(str(size))\n        word2vec_file = OUTPUT_FOLDER + '2ata' + '_PAD.model'\n    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n\n    w2v_model.save(word2vec_file)\n    return w2v_model, word2vec_file\n\n# Train Word2vec model\nw2vmodel, word2vec_file = make_word2vec_model(train, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Padding \nThis part adds \"pad\" end of the vectors in order to make all the vectors in the same length."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sen_len = train.stemmed_tokens.map(len).max()\n\npadding_idx = w2vmodel.wv.vocab['pad'].index\n#print(padding_idx)\ndef make_word2vec_vector_cnn(sentence):\n    padded_X = [padding_idx for i in range(max_sen_len)]\n    i = 0\n    for word in sentence:\n        if word not in w2vmodel.wv.vocab:\n            padded_X[i] = 0\n        else:\n            padded_X[i] = w2vmodel.wv.vocab[word].index\n        i += 1\n    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN Classifier Model Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_SIZE = 500\nNUM_FILTERS = 10\nimport gensim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device available for running: \" + str(device))\n\n#torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], \n#stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, \n#dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\n\nclass CnnTextClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n        super(CnnTextClassifier, self).__init__()\n        w2vmodel = gensim.models.KeyedVectors.load(OUTPUT_FOLDER + '2ata_PAD.model')\n        weights = w2vmodel.wv\n        # With pretrained embeddings\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n        \n        # like a python list, it was designed to store any desired number of nn.Module\n        self.convs = nn.ModuleList([\n                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n                                   for window_size in window_sizes\n        ])\n    \n        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x) # [B, T, E]\n\n        # Apply a convolution + max_pool layer for each window size\n        x = torch.unsqueeze(x, 1)\n        xs = []\n        for conv in self.convs:\n            x2 = torch.tanh(conv(x))\n            x2 = torch.squeeze(x2, -1)\n            x2 = F.max_pool1d(x2, x2.size(2))\n            xs.append(x2)\n        x = torch.cat(xs, 2)\n\n        # FC\n        x = x.view(x.size(0), -1)\n        logits = self.fc(x)\n\n        probs = F.softmax(logits, dim = 1)\n\n        return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_target(label):\n    if label == 0:\n        return torch.tensor([0], dtype=torch.long, device=device)\n    elif label == 1:\n        return torch.tensor([1], dtype=torch.long, device=device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_CLASSES = 2\nVOCAB_SIZE = len(w2vmodel.wv.vocab)\nprint(VOCAB_SIZE)\ncnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\ncnn_model.to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)\nnum_epochs = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Open the file for writing loss\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nf = open(loss_file_name,'w')\nf.write('iter, loss')\nf.write('\\n')\nlosses = []\n\ncnn_model.train()\nfor epoch in range(num_epochs):\n    print(\"Epoch\" + str(epoch + 1))\n    train_loss = 0\n    for index, row in X_train.iterrows():\n        # Clearing the accumulated gradients\n        cnn_model.zero_grad()\n\n        # Make the bag of words vector for stemmed tokens \n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n       \n        # Forward pass to get output\n        probs = cnn_model(bow_vec)\n\n        # Get the target label\n        #print(Y_train['label'][index])\n        target = make_target(Y_train['label'][index])\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = loss_function(probs, target)\n        train_loss += loss.item()\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n    print(f'train_loss : {train_loss / len(X_train)}')\n    print(\"Epoch ran :\"+ str(epoch+1))\n    \n    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n    f.write('\\n')\n    train_loss = 0\n\ntorch.save(cnn_model, OUTPUT_FOLDER + 'cnn_big_model_500_with_padding.pth')\n\nf.close()\nprint(\"Input vector\")\n#print(bow_vec.cpu().numpy())\nprint(\"Probs\")\nprint(probs)\nprint(torch.argmax(probs, dim=1).cpu().numpy()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nbow_cnn_predictions = []\noriginal_lables_cnn_bow = []\ncnn_model.eval()\nloss_df = pd.read_csv(OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv')\nprint(\"atataa\")\nprint(loss_df.columns)\n# loss_df.plot('loss')\n\ny_pred_list = []\ny_true_list = []\n\nwith torch.no_grad():\n    for index, row in X_test.iterrows():\n        #print(row['stemmed_tokens'])\n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n        #print(bow_vec)\n        probs = cnn_model(bow_vec)\n        #print(probs.data)\n        _, predicted = torch.max(probs.data,  1)\n        \n        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n        original_lables_cnn_bow.append(make_target(Y_test['label'][index]).cpu().numpy()[0])\n\nprint(confusion_matrix(original_lables_cnn_bow, bow_cnn_predictions))\n#print(original_lables_cnn_bow)\nprint(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nloss_df = pd.read_csv(loss_file_name)\nprint(loss_df.columns)\nplt_500_padding_30_epochs = loss_df[' loss'].plot()\nfig = plt_500_padding_30_epochs.get_figure()\nfig.savefig(OUTPUT_FOLDER + '1loss_plt_500_padding_30_epochs.pdf')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}